---
title: "Laplace Smoothing Method"
subtitle: _(Part 2 of Modelling Textual Data Series)_
author: 
  - Rey R. Cuenca^[Department of Mathematics and Statistics, MSU-Iligan Institute of Technology]
date: "May 2022"
abstract: |
  This paper gives a quick introduction about the Laplace smoothing method on how to deal with zero counts in language model estimation.
output:
  bookdown::pdf_document2:
    template: pandoc/rvc-template.tex
    extra_dependencies: ["float"]
    keep_tex: true
geometry:
  - margin=1.25in
fontsize: 12pt
header-includes:
  - \usepackage{tgpagella}
  - \usepackage{float}
  - \usepackage{caption}
  - \captionsetup{width=0.85\linewidth, font=footnotesize, labelfont=bf}
  - \newcommand{\w}{\mathbf{w}}
  - \newcommand{\dd}{\text{d}}
  - \newcommand{\tc}[1]{\texttt{#1}}
  - \newcommand{\mb}[1]{\mathbf{#1}}
  - \newcommand{\x}{\mathbf{x}}
  - \newcommand{\y}{\mathbf{y}}
  - \newcommand{\p}{\mathbf{p}}
  - \newcommand{\ds}{\displaystyle}
  - \newcommand{\V}{\mathcal{V}}
  - \newcommand{\T}{\mathcal{T}}
toc: false
indent: false
classoption:
  a4paper
bibliography: references.bib
csl: apa-5th-edition.csl
link-citations: yes
linkcolor: blue
citecolor: blue
urlcolor: blue
---
```{r echo=FALSE}
knitr::opts_chunk$set(comment = "#>")
```


# Introduction

Recall from the previous lesson that rarity of particular token sequence lead to zero counts during the estimation of probabilities. As shown this is very common for $n$-gram models that are of longer lengths like the following:
$$
C(\tc{in},\tc{the},\tc{garden},\tc{just},\tc{outside},\tc{city}) = 0
$$
Such counts lead to underestimation of the true probabilities. Thus, we discuss in this article a collection of methods called _smoothing_ to address this issue. 
<!-- The discussion will be quite straightforward with the mathematical representation of each methods and algorithm are the only thing that is presented. -->

<!-- # Notations -->

# Dictionaries and OOVs

Let $\T$ be all tokens the in the training set with $|\T| = N$. Futhermore, let $\V$ be the collection of all unique words in $\T$ such that $|\V| = V$. We call $\V$ as the _dictionary_ or _vocabulary_. Tokens that our found in the test but does not belong in $\V$ are what we call _unkown words_ or _out of vocabulary_ (OOV) words.

<!-- ## Vector and Matrix operations -->


<!-- ::: {.definition name="Hadamard Division"} -->
<!-- Let $\mb{A} = (a_{ij})$ and $\mb{B} = (b_{ij})$ be matrices of the same dimension. Then, the _Hadamard division_, denoted by $\oslash$ is defined as -->
<!-- $$ -->
<!--  \mb{A} \oslash \mb{B} = (c_{ij})\quad \text{where}\quad c_{ij} = \frac{a_{ij}}{b_{ij}} -->
<!-- $$ -->
<!-- given that $b_{ij}\neq 0$ for all $i,j$. -->
<!-- ::: -->

<!-- ::: {.example} -->
<!-- Given -->
<!-- $$ -->
<!-- \mb{A} = \begin{bmatrix} -->
<!--  1 & 5 & 3 \\ -->
<!--  0 & 2 & -2 \\ -->
<!--  1 & 5 & 1  -->
<!-- \end{bmatrix}\quad \text{and}\quad -->
<!-- \mb{B} = \begin{bmatrix} -->
<!--  3 & 5 & -1 \\ -->
<!--  1 & 4 & -2 \\ -->
<!--  6 & -1 & 1 -->
<!-- \end{bmatrix} -->
<!-- $$ -->
<!-- Observe that all entries ($b_{ij}$) of $\mb{B}$ are nonzero and both $\mb{A}$ and $\mb{B}$ are of the same dimension. Thus, -->
<!-- $$ -->
<!-- \mb{A} \oslash \mb{B}  -->
<!-- =  -->
<!-- \begin{bmatrix} -->
<!--  1/3 & 5/5 & 3/(-1) \\ -->
<!--  0/1 & 2/4 & (-2)/2 \\ -->
<!--  1/6 & 5/(-1) & 1/1  -->
<!-- \end{bmatrix} -->
<!-- = -->
<!-- \begin{bmatrix} -->
<!--  \tfrac{1}{3} & 1 & -3 \\ -->
<!--  0 & \frac{1}{2} & -1  \\ -->
<!--  \tfrac{1}{6} & -5 & 1  -->
<!-- \end{bmatrix} -->
<!-- $$ -->
<!-- ::: -->

<!-- More examples can be seen in the [Hadamard division](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)#Analogous_operations){target=_blank} wiki page. -->

<!-- ::: {.definition name='Kronecker Product'} -->
<!-- Let $\mb{A} = (a_{ij})$ and $\mb{B} = (b_{ij})$ be matrices not necessarily of the same dimension. Then, the _Kronecker product_ of $\mb{A}$ and $\mb{B}$ denoted by $\mb{A}\otimes\mb{B}$ is defined as -->
<!-- $$ -->
<!-- \mb{A}\otimes\mb{B} = (a_{ij}\mb{B}). -->
<!-- $$ -->
<!-- ::: -->

<!-- ::: {.example} -->
<!-- Given $$\mb{A} = \begin{bmatrix} 1 & 2 & 2 & -1 \end{bmatrix} \quad \text{and} \quad \mb{B} = \begin{bmatrix} a \\ b \\ c \end{bmatrix}.$$ -->
<!-- Then, -->
<!-- $$ -->
<!-- \begin{aligned} -->
<!-- \mb{A} \otimes \mb{B}  -->
<!-- &= \begin{bmatrix} 1(\mb{B}) & 2(\mb{B}) & 2(\mb{B}) & -1(\mb{B}) \end{bmatrix}  \\ -->
<!-- &= \begin{bmatrix} 1 \begin{bmatrix} a \\ b \\ c \end{bmatrix} & 2\begin{bmatrix} a \\ b \\ c \end{bmatrix} & 2\begin{bmatrix} a \\ b \\ c \end{bmatrix} & -1\begin{bmatrix} a \\ b \\ c \end{bmatrix} \end{bmatrix}  \\ -->
<!-- &= \begin{bmatrix} -->
<!-- a & 2a & 2a & -a \\ -->
<!-- b & 2b & 2b & -b \\ -->
<!-- c & 2c & 2c & -c -->
<!-- \end{bmatrix} -->
<!-- \end{aligned} -->
<!-- $$ -->
<!-- ::: -->

<!-- ::: {.example}  -->
<!-- Given $$\mb{A} = \begin{bmatrix} 1 \\ 2 \\ 2 \\ -1 \end{bmatrix} \quad \text{and} \quad \mb{B} = \begin{bmatrix} a & b \\ c & d \end{bmatrix}.$$ -->
<!-- Then, -->
<!-- $$ -->
<!-- \begin{aligned} -->
<!-- \mb{A} \otimes \mb{B}  -->
<!-- = \begin{bmatrix} 1(\mb{B}) \\ 2(\mb{B}) \\ 2(\mb{B}) \\ -1(\mb{B}) \end{bmatrix}  -->
<!-- = \begin{bmatrix} 1\begin{bmatrix} a & b \\ c & d \end{bmatrix} \\[5pt] 2\begin{bmatrix} a & b \\ c & d \end{bmatrix} \\ 2\begin{bmatrix} a & b \\ c & d \end{bmatrix} \\ -1\begin{bmatrix} a & b \\ c & d \end{bmatrix} \end{bmatrix}  -->
<!-- = \begin{bmatrix} -->
<!-- a & b \\ -->
<!-- c & d \\ -->
<!-- 2a & 2b \\ -->
<!-- 2c & 2d \\ -->
<!-- 2a & 2b \\ -->
<!-- 2c & 2d \\ -->
<!-- -a & -b \\ -->
<!-- -c & -d  -->
<!-- \end{bmatrix} -->
<!-- \end{aligned} -->
<!-- $$ -->
<!-- ::: -->

<!-- ::: {.example} -->
<!-- Given $$\mb{A} = \begin{bmatrix} x & y \\ u & w \end{bmatrix} \quad \text{and} \quad \mb{B} = \begin{bmatrix} a \\ b \\ c \end{bmatrix}.$$ -->
<!-- Then, -->
<!-- $$ -->
<!-- \begin{aligned} -->
<!-- \mb{A} \otimes \mb{B}  -->
<!-- = \begin{bmatrix} x\mb{B} & y\mb{B} \\ u\mb{B} & w\mb{B}  \end{bmatrix}  -->
<!-- = \begin{bmatrix} x\begin{bmatrix} a \\ b \\ c \end{bmatrix} & y\begin{bmatrix} a \\ b \\ c \end{bmatrix} \\ u\begin{bmatrix} a \\ b \\ c \end{bmatrix} & w\begin{bmatrix} a \\ b \\ c \end{bmatrix}  \end{bmatrix}  -->
<!-- = \begin{bmatrix} -->
<!-- xa & ya \\ -->
<!-- xb & yb \\ -->
<!-- xc & yc \\ -->
<!-- ua & wa \\ -->
<!-- ub & wb \\ -->
<!-- uc & wc \\ -->
<!-- \end{bmatrix} -->
<!-- \end{aligned} -->
<!-- $$ -->
<!-- ::: -->

<!-- More examples can be seen in the [Kronecker product](https://en.wikipedia.org/wiki/Kronecker_product#Definition){target=_blank} wiki page. -->

# Laplace Smoothing

Recall that we estimate the $n$-gram probabilities $P(w_k|\w_{(k-n+1):(k-1)})$ as follows
\begin{equation}
P(w_k|\w_{(k-n+1):(k-1)}) = \frac{C(\w_{(k-n+1:k)})}{C(\w_{(k-n+1):(k-1)})}
(\#eq:ngram-prob)
\end{equation}
The common problem of zero counts is when the numerator of the estimator above is zero, i.e., $C(\w_{(k-n+1):k})=0$. One way to solve this is by adding $1$ to the numerator count, i.e.,
$$
C(\w_{(k-n+1):k}) \longrightarrow C(\w_{(k-n+1):k}) + 1
$$
However, we also need to adjust the denominator in order for new $P_{\text{LP}}$ to be still a probability. To do this, note that 
$$
P(w_k|\w_{(k-n+1):(k-1)}) 
= \frac{C(\w_{(k-n+1):k)})}{C(\w_{(k-n+1):(k-1)})}
= \frac{C(\w_{(k-n+1):k})}{\sum_{w\in\V} C(\w_{(k-n+1):(k-1)}w)}
$$
Thus, 

$$
\begin{aligned}
P_{\text{LP}}(w_k|\w_{(k-n+1):(k-1)})  
&= \frac{C(\w_{(k-n+1):k}) + 1}{\sum_{w\in\V}\left[ C(\w_{(k-n+1):k}) + 1 \right]} \\
&=  \frac{C(\w_{(k-n+1):k}) + 1}{\sum_{w\in\V} C(\w_{(k-n+1):k}) + \sum_{w\in\V}1 }  \\
&=  \frac{C(\w_{(k-n+1):k}) + 1}{C(\w_{(k-n+1):(k-1)}) + V }
\end{aligned}
$$
Therefore, _Laplace smoothed_ estimate of the probability is 
$$
P_{\text{LP}}(w_k|\w_{(k-n+1):(k-1)}) = \frac{C(\w_{(k-n+1):k}) + 1}{C(\w_{(k-n+1):(k-1)}) + V }.
$$


::: {.example}
Now let us consider some examples illustrating the Laplace smoothing method.
Weâ€™ll use data from the now-defunct Berkeley Restaurant Project,
a dialogue system from the last century that answered questions about a database
of restaurants in Berkeley, California [@Jurafsky1994TheBR]. For the current illustration, we use only 8 tokens from the corpus with unigram and bigram counts tabulated respectively in Tables \@ref(tab:berkeley-unigram) and \@ref(tab:berkeley-bigram). This tables are saved as csv files in `/data/berkeley-unigram.csv` and `/data/berkeley-bigram.csv`.

```{r berkeley-unigram, echo=FALSE, fig.align='center'}
suppressMessages(suppressWarnings(library(tidyverse)))
read.csv("data/berkeley-unigram.csv") %>% 
  `colnames<-`(c("",colnames(.)[-1])) %>% 
  "["(,-1) %>% 
  `rownames<-`(NULL) -> unigram_dt 

unigram_dt %>% 
  knitr::kable(booktabs=TRUE, caption= 'Unigram counts for eight of the wordsin the Berkeley Restaurant Project corpus of 9332 sentences', longtable=TRUE) %>% 
  kableExtra::kable_styling(position = "center")
```

```{r berkeley-bigram, echo=FALSE, fig.align='center'}

read.csv("data/berkeley-bigram.csv") %>% 
  `colnames<-`(c("",colnames(.)[-1])) -> bigram_dt 

bigram_dt %>% 
  knitr::kable(booktabs=TRUE, caption= 'Bigram counts for eight of the words in the Berkeley Restaurant Project corpus of 9332 sentences', longtable=TRUE) %>% 
  kableExtra::kable_styling(position = "center")
```


Our goal is to fill in these zeros in the bigram counts. First we note that we are restricting our vocabularies to 8 words thus
$$
\V = \{`r unigram_dt %>% names %>% str_c('\\tc{',.,'}',collapse=',')`\}.
$$
and $|\V| = V = 8$. For simplicity, let us denote as matrix $W$ the bigram counts in Table  \@ref(tab:berkeley-bigram), i.e., 
$$
W = 
\begin{bmatrix}
       5 & 827 & - &   9 & - & - & - &   2 \\ 
    2 & - & 608 &   1 &   6 &   6 &   5 &   1 \\ 
    2 & - &   4 & 686 &   2 & - &   6 & 211 \\ 
  - & - &   2 & - &  16 &   2 &  42 & - \\ 
    1 & - & - & - & - &  82 &   1 & - \\ 
   15 & - &  15 & - &   1 &   4 & - & - \\ 
    2 & - & - & - & - &   1 & - & - \\ 
    1 & - &   1 & - & - & - & - & - 
\end{bmatrix}
$$
where "$-$" denotes zero counts and $W = (C(w_i,w_j))$ with $w_i,w_j \in \V$. Without smoothing the bigram probabilities are derived by dividing element-wise the values of Table \@ref(tab:berkeley-unigram) to each of the rows of $W$ to arrive with the following matrix $P_b$.

$$
P_b =
\begin{bmatrix}
0.002 & 0.33 & - & 0.0036 & - & - & - & 0.00079 \\ 
  0.0022 & - & 0.66 & 0.0011 & 0.0065 & 0.0065 & 0.0054 & 0.0011 \\ 
  0.00083 & - & 0.0017 & 0.28 & 0.00083 & - & 0.0025 & 0.087 \\ 
  - & - & 0.0027 & - & 0.021 & 0.0027 & 0.056 & - \\ 
  0.0063 & - & - & - & - & 0.52 & 0.0063 & - \\ 
  0.014 & - & 0.014 & - & 0.00091 & 0.0037 & - & - \\ 
  0.0059 & - & - & - & - & 0.0029 & - & - \\ 
  0.0036 & - & 0.0036 & - & - & - & - & - 
\end{bmatrix}
$$
Note that $P_b = (P_{ij})$ where $(i,j)$-th entry is $P_{ij} = C(w_i,w_j)/C(w_i)= P(w_j|w_i)$. For example, from matrix $P_b$, we can see the following probabilities

$$P(\tc{i}|\tc{i}) = 0.002\quad \text{and} \quad P(\tc{want}|\tc{i}) = 0.33.$$
Now, let us try to smooth matrix $P_b$ using Laplace smoothing. To do this, first we add 1 to each of the entries in $W$ and denote the resulting matrix as $W^*$,
$$
W^* = W + 1 
= 
\left[
\begin{array}{rrrrrrrr}
    6 & 828 &   1 &  10 &   1 &   1 &   1 &   3 \\ 
    3 &   1 & 609 &   2 &   7 &   7 &   6 &   2 \\ 
    3 &   1 &   5 & 687 &   3 &   1 &   7 & 212 \\ 
    1 &   1 &   3 &   1 &  17 &   3 &  43 &   1 \\ 
    2 &   1 &   1 &   1 &   1 &  83 &   2 &   1 \\ 
   16 &   1 &  16 &   1 &   2 &   5 &   1 &   1 \\ 
    3 &   1 &   1 &   1 &   1 &   2 &   1 &   1 \\ 
    2 &   1 &   2 &   1 &   1 &   1 &   1 &   1 \\ 
\end{array}
\right]
$$
Next is we divide element-wise the values of Table \@ref(tab:berkeley-unigram) to each of the rows of $W^*$ but this time with additional term in the divisor $V=8$. If we denote the smoothed bigram probability matrix as $P_{\text{LP}} = (P^*_{ij})$, then 
$$
P^*_{ij} = \frac{C(w_i,w_j) + 1}{C(w_i) + 8}
$$
Thus, matrix $P_{\text{LP}}$ of Laplace-smoothed probabilities is
$$
P_b 
=
\begin{bmatrix}
  0.0024 & 0.33 & 0.00039 & 0.0039 & 0.00039 & 0.00039 & 0.00039 & 0.0012 \\ 
  0.0032 & 0.0011 & 0.65 & 0.0021 & 0.0075 & 0.0075 & 0.0064 & 0.0021 \\ 
  0.0012 & 0.00041 & 0.0021 & 0.28 & 0.0012 & 0.00041 & 0.0029 & 0.087 \\ 
  0.0013 & 0.0013 & 0.004 & 0.0013 & 0.023 & 0.004 & 0.057 & 0.0013 \\ 
  0.012 & 0.006 & 0.006 & 0.006 & 0.006 & 0.5 & 0.012 & 0.006 \\ 
  0.015 & 0.00091 & 0.015 & 0.00091 & 0.0018 & 0.0045 & 0.00091 & 0.00091 \\ 
  0.0086 & 0.0029 & 0.0029 & 0.0029 & 0.0029 & 0.0057 & 0.0029 & 0.0029 \\ 
  0.007 & 0.0035 & 0.007 & 0.0035 & 0.0035 & 0.0035 & 0.0035 & 0.0035
  \end{bmatrix}
$$

Indicating the tokens gives us Table \@ref(tab:bigram-smooth).

```{r bigram-smooth, echo=FALSE, eval=TRUE}
bigram_dt %>% "["(,-1) %>%  
  "+"(1L) %>%
  as.matrix %>% 
  "%*%"(diag(1/(unigram_dt+8)),.) %>%
  formatC(format='g', digits = 2, zero.print = "-") %>%
  as.data.frame() %>% 
  `rownames<-`(colnames(.)) %>% 
  knitr::kable(booktabs=TRUE, caption= 'New bigram probabilities smoothed using Laplace method.', longtable=TRUE) %>% 
  kableExtra::kable_styling(position = "center")
```

With smoothed probabilities,

$$P(\tc{i}|\tc{i}) = 0.0024\quad \text{and} \quad P(\tc{to}|\tc{i}) = 0.00039.$$


```{r echo=FALSE, eval=FALSE}
bigram_dt %>% 
  "["(,-1) %>% 
  "+"(1L)  %>%
  as.matrix %>% 
  xtable::xtableMatharray(.)
```

:::


# Other Methods

There are many other smoothing methods not included in this discussion paper. For other methods, you can refer to [@snlp2021, p.42].



# References